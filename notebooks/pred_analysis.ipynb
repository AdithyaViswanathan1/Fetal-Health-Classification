{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = joblib.load('../models/best_decision_tree_model.joblib')\n",
    "knn_model = joblib.load('../models/best_knn_model.joblib')\n",
    "best_rf_model = joblib.load('../models/best_random_forest_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('../data/x_train.npy')\n",
    "x_test = np.load('../data/x_test.npy')\n",
    "y_train = np.load('../data/y_train.npy')\n",
    "y_test = np.load('../data/y_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "- take the best model\n",
    "- do the following on training data and then testing data\n",
    "- see the distribution of predicted classes (how many were predicted 1, 2, and 3?)\n",
    "- see only where the predictions were incorrect\n",
    "- see any patterns in wrong predictions (ex: when actual answer was 1, it always predicted 2)\n",
    "- then create custom error rates \n",
    "- continue creating custom metrics to give insights into model weaknesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = best_rf_model\n",
    "y_pred = model.predict(x_test)\n",
    "print(y_pred[:20])\n",
    "print(y_test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Distribution:\n",
      "1.0    338\n",
      "2.0     55\n",
      "3.0     33\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Actual Distribution:\n",
      "1.0    334\n",
      "2.0     57\n",
      "3.0     35\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction Distribution:\\n{pd.Series(y_pred).value_counts()}\")\n",
    "print(f\"\\nActual Distribution:\\n{pd.Series(y_test).value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "Some instances which are 2.0 or 3.0 category are being classified as 1.0. In other words, fetuses which are suspect and pathological level (unhealthy) are being classified as healthy. These are examples of false negatives, which are very dangerous, especially in this case.\n",
    "\n",
    "The effect of false negatives, as discussed in `modeling.ipynb`, is the endangerment of the fetus by not realizing an unhealthy fetus needs medical intervention.\n",
    "\n",
    "Let's see where the classification is going wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Misclassifications: 24\n",
      "{'Prediction: 2.0 | True: 1.0': 8, 'Prediction: 1.0 | True: 2.0': 12, 'Prediction: 2.0 | True: 3.0': 2, 'Prediction: 3.0 | True: 1.0': 1, 'Prediction: 1.0 | True: 3.0': 1}\n",
      "Most prevalent misclassifiction: Prediction: 1.0 | True: 2.0\n"
     ]
    }
   ],
   "source": [
    "error_count = 0\n",
    "type_of_error = {}\n",
    "for pred, true in zip(y_pred, y_test):\n",
    "    if pred != true:\n",
    "        key = f\"Prediction: {pred} | True: {true}\"\n",
    "        # print(key)\n",
    "        if key not in type_of_error.keys():\n",
    "            type_of_error[key] = 1\n",
    "        else:\n",
    "            type_of_error[key] += 1\n",
    "        error_count+=1\n",
    "\n",
    "print(f\"# Misclassifications: {error_count}\")\n",
    "print(type_of_error)\n",
    "key_with_max_value = max(type_of_error, key=type_of_error.get)\n",
    "print(f\"Most prevalent misclassifiction: {key_with_max_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "The types of misclassifications are:\n",
    "\n",
    "Type 1: Predicting 1 when truly 2 => 12 occurences\n",
    "\n",
    "Type 2: Predicting 2 when truly 1 => 8 occurences\n",
    "\n",
    "Type 3: Predicting 2 when truly 3 => 2 occurences\n",
    "\n",
    "Type 4: Predicting 3 when truly 1 => 1 occurence\n",
    "\n",
    "Type 5: Predicting 1 when truly 3 => 1 occurence\n",
    "<br><br>\n",
    "\n",
    "This can be translated to:\n",
    "\n",
    "Type 1: Predicting HEALTHY when SUSPECT => 12 occurences (***)\n",
    "\n",
    "Type 2: Predicting SUSPECT when HEALTHY => 8 occurences\n",
    "\n",
    "Type 3: Predicting SUSPECT when PATHOLOGICAL => 2 occurences\n",
    "\n",
    "Type 4: Predicting PATHOLOGICAL when HEALTHY => 1 occurence\n",
    "\n",
    "Type 5: Predicting HEALTHY when PATHOLOGICAL => 1 occurence (***)\n",
    "\n",
    "_(***) indicates dangerous misclassifications_\n",
    "<br><br>\n",
    "\n",
    "Let's look into Type 1 misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify previous code to keep track of pred and true value for each index in y_pred and y_true. \n",
    "# pull those cases from x_test and see for any patterns in misclassification type 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
